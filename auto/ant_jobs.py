#!/usr/bin/env python3
"""
èš‚èšé›†å›¢æ ¡å›­æ‹›è˜çˆ¬è™«ï¼ˆæœ€ç»ˆæ•´åˆç‰ˆï¼‰
æ”¯æŒåˆ†é¡µ + æ–°Tabè¯¦æƒ…é¡µæŠ“å–èŒä½æè¿°/èŒä½è¦æ±‚
"""

import asyncio
from playwright.async_api import async_playwright, Page
import json
import csv
from datetime import datetime
import time
from typing import List, Dict


# ===============================================================
# CSS é€‰æ‹©å™¨é…ç½®
# ===============================================================
SELECTORS = {
    'job_list': 'ul.ant-list-items',
    'job_item': 'li.ant-list-item',
    'job_name': '[class*="item-name"]',
    'job_category': '[class*="item-description"] .ant-typography',
    'job_location': '[class*="item-actions-content"]',
    'job_tag': '[class*="item-tag"]',
}


# ===============================================================
# æŠ“å–èŒä½è¯¦æƒ…é¡µï¼ˆèŒä½æè¿° + èŒä½è¦æ±‚ï¼‰
# ===============================================================
async def get_job_details(page: Page) -> dict:
    """
    ä»èŒä½è¯¦æƒ…é¡µä¸­æå–ã€èŒä½æè¿°ã€‘å’Œã€èŒä½è¦æ±‚ã€‘
    ç¬¬ä¸€ä¸ª [class^="positionDetailConditions"] ä¸ºèŒä½æè¿°
    ç¬¬äºŒä¸ª [class^="positionDetailConditions"] ä¸ºèŒä½è¦æ±‚
    """
    await page.wait_for_selector('[class^="positionDetailConditions"]', timeout=15000)
    await asyncio.sleep(1.2)

    try:
        sections = await page.query_selector_all('[class^="positionDetailConditions"]')
        job_desc, job_req = "", ""

        if len(sections) >= 1:
            desc_el = await sections[0].query_selector('section')
            if desc_el:
                job_desc = (await desc_el.inner_text()).strip()

        if len(sections) >= 2:
            req_el = await sections[1].query_selector('section')
            if req_el:
                job_req = (await req_el.inner_text()).strip()

        return {"èŒä½æè¿°": job_desc, "èŒä½è¦æ±‚": job_req}

    except Exception as e:
        print(f"âš ï¸ æŠ“å–èŒä½è¯¦æƒ…å¤±è´¥: {e}")
        return {"èŒä½æè¿°": "", "èŒä½è¦æ±‚": ""}


# ===============================================================
# è§£æå½“å‰é¡µèŒä½ï¼ˆå¹¶è¿›å…¥è¯¦æƒ…é¡µï¼‰
# ===============================================================
async def parse_current_page(page: Page, current_page: int) -> List[Dict]:
    await page.wait_for_selector(SELECTORS['job_list'], timeout=10000)
    await asyncio.sleep(1)

    job_items = await page.query_selector_all(SELECTORS['job_item'])
    print(f"ğŸ“‹ ç¬¬ {current_page} é¡µæ£€æµ‹åˆ° {len(job_items)} ä¸ªèŒä½")

    jobs = []

    for index, item in enumerate(job_items):
        try:
            # åŸºç¡€ä¿¡æ¯
            name_el = await item.query_selector(SELECTORS['job_name'])
            job_name = (await name_el.inner_text()).strip() if name_el else ""
            if not job_name:
                continue

            category_el = await item.query_selector(SELECTORS['job_category'])
            job_category = (await category_el.inner_text()).strip() if category_el else ""

            location_el = await item.query_selector(SELECTORS['job_location'])
            job_location = (await location_el.inner_text()).strip() if location_el else ""

            tag_els = await item.query_selector_all(SELECTORS['job_tag'])
            tags = list({(await t.inner_text()).strip() for t in tag_els if (await t.inner_text()).strip()})

            # æ‰“å¼€è¯¦æƒ…é¡µ
            print(f"  â†’ æ‰“å¼€è¯¦æƒ…é¡µ: {job_name}")
            async with page.context.expect_page() as new_page_info:
                await item.click()
            detail_page = await new_page_info.value

            await detail_page.wait_for_load_state("networkidle")
            await asyncio.sleep(1.2)
            detail_link = detail_page.url
            details = await get_job_details(detail_page)
            await detail_page.close()

            jobs.append({
                "èŒä½": job_name,
                "å…¬å¸": "èš‚èšé›†å›¢",
                "ç±»åˆ«": job_category,
                "å·¥ä½œåœ°ç‚¹": job_location,
                "æ ‡ç­¾": ", ".join(tags),
                "é“¾æ¥": detail_link,
                **details,
            })

            await asyncio.sleep(0.8)

        except Exception as e:
            print(f"âŒ ç¬¬ {index+1} ä¸ªèŒä½æŠ“å–å¤±è´¥: {e}")
            continue

    print(f"âœ… ç¬¬ {current_page} é¡µæŠ“å–å®Œæˆï¼Œå…± {len(jobs)} ä¸ªèŒä½")
    return jobs


# ===============================================================
# åˆ†é¡µé€»è¾‘ï¼šè·å–æ€»é¡µæ•° + è·³è½¬
# ===============================================================
async def get_total_pages(page: Page) -> int:
    """æ ¹æ®åˆ†é¡µç»“æ„æå–æ€»é¡µæ•°"""
    try:
        await page.wait_for_selector('div.ant-list-pagination ul.ant-pagination', timeout=8000)
        page_items = await page.query_selector_all('ul.ant-pagination li.ant-pagination-item')
        last_page = 1
        for item in page_items:
            title = await item.get_attribute("title")
            if title and title.isdigit():
                last_page = max(last_page, int(title))
        print(f"ğŸ“„ æ£€æµ‹åˆ°æ€»é¡µæ•°: {last_page}")
        return last_page
    except Exception as e:
        print(f"âš ï¸ è·å–æ€»é¡µæ•°å¤±è´¥: {e}")
        return 1


async def go_to_page(page: Page, target_page: int) -> bool:
    """è·³è½¬åˆ°æŒ‡å®šé¡µ"""
    try:
        await page.wait_for_selector('ul.ant-pagination', timeout=8000)

        # å½“å‰é¡µ
        active = await page.query_selector('li.ant-pagination-item-active')
        current_title = await active.get_attribute("title") if active else None
        if current_title == str(target_page):
            print(f"âœ… å·²åœ¨ç¬¬ {target_page} é¡µ")
            return True

        # ç‚¹å‡»ç›®æ ‡é¡µ
        target_item = await page.query_selector(f'li.ant-pagination-item[title="{target_page}"]')
        if target_item:
            print(f"â¡ï¸ ç‚¹å‡»ç¬¬ {target_page} é¡µ")
            await target_item.click()
        else:
            next_btn = await page.query_selector('li.ant-pagination-next:not([aria-disabled="true"])')
            if next_btn:
                print("â¡ï¸ ç‚¹å‡»ä¸‹ä¸€é¡µ")
                await next_btn.click()
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ç›®æ ‡é¡µæˆ–ä¸‹ä¸€é¡µæŒ‰é’®")
                return False

        await asyncio.sleep(2)
        await page.wait_for_selector(SELECTORS['job_list'], timeout=10000)
        print(f"âœ… æˆåŠŸè·³è½¬åˆ°ç¬¬ {target_page} é¡µ")
        return True

    except Exception as e:
        print(f"âš ï¸ è·³è½¬åˆ°ç¬¬ {target_page} é¡µå¤±è´¥: {e}")
        return False


# ===============================================================
# ä¸»é€»è¾‘
# ===============================================================
async def scrape_all_pages(url: str, max_pages: int = None, headless: bool = False):
    all_jobs = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=headless)
        page = await browser.new_page()
        await page.goto(url, wait_until="networkidle", timeout=60000)
        await asyncio.sleep(2)

        total_pages = await get_total_pages(page)
        pages_to_scrape = min(total_pages, max_pages) if max_pages else total_pages
        print(f"\nğŸŒ æ€»é¡µæ•°: {total_pages}ï¼Œè®¡åˆ’æŠ“å–: {pages_to_scrape} é¡µ\n")

        for page_num in range(1, pages_to_scrape + 1):
            print(f"\n================ ç¬¬ {page_num} é¡µ ================")
            jobs = await parse_current_page(page, page_num)
            all_jobs.extend(jobs)

            if page_num < pages_to_scrape:
                success = await go_to_page(page, page_num + 1)
                if not success:
                    break

        await browser.close()

    return all_jobs


# ===============================================================
# ä¿å­˜ç»“æœ
# ===============================================================
def save_to_csv(jobs: List[Dict], filename: str):
    if not jobs:
        return
    with open(filename, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=jobs[0].keys())
        writer.writeheader()
        writer.writerows(jobs)


def save_to_json(jobs: List[Dict], filename: str):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(jobs, f, ensure_ascii=False, indent=2)


# ===============================================================
# å…¥å£
# ===============================================================
async def main():
    URL = "https://talent.antgroup.com/campus-full-list?type=campus_graduates"
    HEADLESS = False     # è®¾ä¸º True å¯åå°è¿è¡Œ
    MAX_PAGES = None        # æ”¹ä¸º None æŠ“å–å…¨éƒ¨é¡µ

    print("=" * 70)
    print("ğŸš€ èš‚èšé›†å›¢æ ¡å›­æ‹›è˜çˆ¬è™«å¯åŠ¨")
    print(f"URL: {URL}")
    print(f"æœ€å¤§é¡µæ•°: {MAX_PAGES}")
    print(f"æ— å¤´æ¨¡å¼: {HEADLESS}")
    print("=" * 70)

    start = time.time()
    jobs = await scrape_all_pages(URL, MAX_PAGES, HEADLESS)
    end = time.time()

    print(f"\nâœ… æŠ“å–å®Œæˆï¼Œå…± {len(jobs)} æ¡ï¼Œç”¨æ—¶ {end - start:.2f} ç§’")

    if jobs:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_name = f"ant_jobs.csv"
        json_name = f"ant_jobs.json"
        save_to_csv(jobs, csv_name)
        save_to_json(jobs, json_name)
        print(f"\nğŸ“ æ•°æ®å·²ä¿å­˜ï¼š\n  CSV: {csv_name}\n  JSON: {json_name}")

    print("\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(main())
