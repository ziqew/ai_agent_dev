import asyncio
import pandas as pd
from playwright.async_api import async_playwright, TimeoutError
import random

BASE_URL = "https://join.qq.com/post.html?query=p_1,w_1,w_2,w_5,w_3,w_8,w_6,w_37,w_14,w_31,w_17,w_7,w_30,w_11,w_9&c_t=1"

async def extract_detail(context, detail_page):
    """è§£æèŒä½è¯¦æƒ…é¡µå†…å®¹"""
    try:
        await detail_page.wait_for_selector(".post_detail", timeout=10000)
    except TimeoutError:
        return {"desc": "", "req": "", "plus": ""}

    # æå–æ‰€æœ‰ detail_box
    boxes = await detail_page.query_selector_all(".post_detail .detail_box")
    detail_data = {"desc": "", "req": "", "plus": ""}

    for box in boxes:
        subtitle = (await (await box.query_selector(".subtitle")).inner_text()).strip()
        text_box = await box.query_selector(".text_box .detail_text") or await box.query_selector(".detail_text")
        text = (await text_box.inner_text()).strip() if text_box else ""

        if "æè¿°" in subtitle:
            detail_data["desc"] = text
        elif "è¦æ±‚" in subtitle:
            detail_data["req"] = text
        elif "åŠ åˆ†" in subtitle or "æ³¨æ„" in subtitle:
            detail_data["plus"] = text

    return detail_data


async def extract_jobs(page, context):
    """æå–å½“å‰é¡µèŒä½åˆ—è¡¨ï¼Œå¹¶æ‰“å¼€è¯¦æƒ…é¡µæŠ“å–å†…å®¹"""
    await page.wait_for_selector("ul.post_list li.post_box", timeout=10000)
    jobs = []
    cards = await page.query_selector_all("ul.post_list li.post_box")

    for idx, c in enumerate(cards):
        title = (await (await c.query_selector(".post_title")).inner_text()).strip()
        category = (await (await c.query_selector(".post_tag_box .post_tag")).inner_text()).strip()
        site = (await (await c.query_selector(".site_box .site")).inner_text()).strip()

        tags_el = await c.query_selector_all(".post_tag_box .post_tag")
        tags = [ (await e.inner_text()).strip().replace("ï½œ", "").strip() for e in tags_el[1:] ]
        tags_text = " | ".join(tags)

        # æ‰“å¼€æ–°æ ‡ç­¾é¡µ
        async with context.expect_page() as new_page_info:
            await c.click()
        detail_page = await new_page_info.value

        # æå–è¯¦æƒ…å†…å®¹
        detail_link = detail_page.url
        detail_data = await extract_detail(context, detail_page)

        await detail_page.close()
        await asyncio.sleep(random.uniform(0.8, 1.6))

        jobs.append({
            "title": title,
            "company": "è…¾è®¯",
            "category": category,
            "tags": tags_text,
            "site": " ".join(site.split()),
            "link":detail_link,
            "desc": detail_data["desc"],
            "requirement": detail_data["req"],
            "plus": detail_data["plus"],
        })
        print(f"  âœ… [{idx+1}/{len(cards)}] {title}")

    return jobs


async def crawl_all(output="tencent_jobs.csv"):
    async with async_playwright() as pw:
        browser = await pw.chromium.launch(headless=False)
        context = await browser.new_context()
        page = await context.new_page()
        await page.goto(BASE_URL, wait_until="domcontentloaded")

        all_jobs = []
        page_index = 1

        while True:
            print(f"\nğŸŸ¢ æŠ“å–ç¬¬ {page_index} é¡µ")
            jobs = await extract_jobs(page, context)
            print(f"  â®• ç¬¬ {page_index} é¡µå…± {len(jobs)} æ¡èŒä½")
            all_jobs.extend(jobs)

            # ç¿»é¡µé€»è¾‘
            next_btn = await page.query_selector(".el-pagination .btn-next")
            if not next_btn:
                print("âš ï¸ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œåœæ­¢ã€‚")
                break

            disabled = await next_btn.get_attribute("disabled")
            if disabled is not None:
                print("âœ… å·²åˆ°æœ€åä¸€é¡µï¼Œç»“æŸæŠ“å–ã€‚")
                break

            await next_btn.click()
            page_index += 1

            # ç­‰å¾…åˆ†é¡µåˆ·æ–°
            await page.wait_for_timeout(2000)
            await page.wait_for_selector(f".el-pager li.number.active:text-is('{page_index}')", timeout=10000)

        df = pd.DataFrame(all_jobs)
        df.to_csv(output, index=False, encoding="utf-8-sig")
        print(f"\nâœ… å…±æŠ“å– {len(all_jobs)} æ¡èŒä½ï¼Œä¿å­˜åˆ° {output}")

        await browser.close()

def convert_csv_text():
    # 1. è¾“å…¥ / è¾“å‡ºæ–‡ä»¶å
    input_csv = "tencent_jobs.csv"     # ä½ çš„æºæ–‡ä»¶
    output_txt = "tencent_jobs.txt"    # ç”Ÿæˆçš„txtæ–‡ä»¶

    # 2. è¯»å– CSV
    df = pd.read_csv(input_csv)

    # 3. ç”Ÿæˆæ–‡æœ¬å†…å®¹
    records = []
    for _, row in df.iterrows():
        job_text = (
            f"ã€èŒä½ã€‘{row['èŒä½']}\n"
            f"ã€å…¬å¸ã€‘{row['å…¬å¸']}\n"
            f"ã€ç±»åˆ«ã€‘{row['ç±»åˆ«']}\n"
            f"ã€æ ‡ç­¾ã€‘{row['æ ‡ç­¾']}\n"
            f"ã€å·¥ä½œåœ°ç‚¹ã€‘{row['å·¥ä½œåœ°ç‚¹']}\n"
            f"ã€å²—ä½æè¿°ã€‘\n{row['å²—ä½æè¿°']}\n\n"
            f"ã€å²—ä½è¦æ±‚ã€‘\n{row['å²—ä½è¦æ±‚']}\n\n"
            f"ã€åŠ åˆ†é¡¹ã€‘\n{row['åŠ åˆ†é¡¹']}\n"
        )
        records.append(job_text.strip())

    # 4. ç”¨ "---" åˆ†éš”èŒä½
    final_text = "\n---\n".join(records)

    # 5. å†™å…¥æ–‡ä»¶
    with open(output_txt, "w", encoding="utf-8") as f:
        f.write(final_text)

    print(f"âœ… å·²ç”Ÿæˆ {output_txt} ï¼Œå…± {len(records)} æ¡èŒä½ä¿¡æ¯ã€‚")


if __name__ == "__main__":
    convert_csv_text()
    #asyncio.run(crawl_all())
