# scrape_qq_jobs.py
# æŠ“å– join.qq.com èŒä½åˆ—è¡¨ + è¯¦æƒ…é¡µï¼ˆæ–° TAB æ‰“å¼€ è‡ªåŠ¨å…³é—­ï¼‰
# å®Œå…¨é€‚é…ä½ æä¾›çš„ DOM ç»“æ„

import json
import time
from pathlib import Path
from playwright.sync_api import sync_playwright


START_URL = lambda p: f"https://join.qq.com/post.html?query=p_{p}"
WAIT = 1.0
MAX_PAGES = 1


def clean_text(x: str):
    return x.strip().replace("\n", " ").replace("\t", " ").replace("  ", " ")


def extract_detail_section(page):
    """è§£æè¯¦æƒ…é¡µçš„ç»“æ„"""

    detail = {}

    boxes = page.locator("ul.post_detail > li.detail_box")
    count = boxes.count()

    for i in range(count):
        box = boxes.nth(i)

        title_el = box.locator(".subtitle")
        title = clean_text(title_el.text_content()) if title_el.count() else ""

        # ä¸¤ç§ç»“æ„ï¼š
        # 1. <div class="text_box"><p class="detail_text">â€¦</p></div>
        # 2. <li><div class="detail_text">â€¦</div></li>
        text1 = box.locator(".text_box .detail_text")
        text2 = box.locator("> .detail_text")

        if text1.count():
            content = clean_text(text1.text_content())
        elif text2.count():
            content = clean_text(text2.text_content())
        else:
            content = ""

        if title:
            detail[title] = content

    return detail


def scrape_all():
    all_jobs = []

    with sync_playwright() as pw:
        browser = pw.chromium.launch(headless=True)
        context = browser.new_context()

        # åˆ—è¡¨é¡µ
        page = context.new_page()

        for p in range(1, MAX_PAGES + 1):
            url = START_URL(p)
            print(f"\nâ¡ï¸ æŠ“å–åˆ—è¡¨é¡µ: {url}")

            page.goto(url, wait_until="networkidle")
            time.sleep(WAIT)

            rows = page.locator("ul.post_list > li.post_box")
            row_count = rows.count()

            if row_count == 0:
                print("âš ï¸ æ— æ›´å¤šèŒä½ï¼Œåœæ­¢ã€‚")
                break

            print(f"âœ… æ‰¾åˆ° {row_count} æ¡èŒä½")

            for i in range(row_count):
                row = rows.nth(i)

                # èŒä½åç§°
                title = clean_text(row.locator(".post_title").text_content())

                # æ ‡ç­¾
                tag_nodes = row.locator(".post_tag_box .post_tag")
                tags = [
                    clean_text(tag_nodes.nth(j).text_content())
                    for j in range(tag_nodes.count())
                ]

                # å·¥ä½œåœ°ç‚¹
                location = clean_text(row.locator(".site_box .site").text_content())

                job = {
                    "title": title,
                    "tags": tags,
                    "location": location,
                    "list_page": url,
                }

                print(f"   ğŸ” æŠ“å–è¯¦æƒ…é¡µï¼ˆæ–° TABï¼‰: {title}")

                # ç­‰å¾…æ–° tab æ‰“å¼€
                with context.expect_page() as new_tab_info:
                    row.click()  # ç‚¹å‡»åˆ—è¡¨è¡Œ â†’ æ‰“å¼€æ–°TAB

                detail_page = new_tab_info.value

                # ç­‰å¾…åŠ è½½
                detail_page.wait_for_load_state("networkidle")
                time.sleep(WAIT)

                # æå–è¯¦æƒ…
                detail_data = extract_detail_section(detail_page)
                job["detail"] = detail_data

                # å…³é—­è¯¦æƒ…é¡µ tab
                detail_page.close()

                all_jobs.append(job)

            if row_count < 10:
                print("ğŸ“Œ æœ€åä¸€é¡µï¼Œä»»åŠ¡ç»“æŸã€‚")
                break

        browser.close()

    return all_jobs


if __name__ == "__main__":
    data = scrape_all()
    out = Path("qq_jobs.json")
    out.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

    print(f"\nâœ… å…±æŠ“å– {len(data)} æ¡èŒä½ï¼ˆå«è¯¦æƒ…ï¼‰")
    print(f"ğŸ“„ å·²å†™å…¥æ–‡ä»¶ï¼š{out.absolute()}")
