import os
import re
import json
import time
import csv
from pathlib import Path
from typing import List, Optional
from dotenv import load_dotenv
import asyncio
from pydantic import BaseModel, Field

from browser_use import Agent,Browser, ChatOpenAI

load_dotenv()
qwen_key = os.getenv("QWEN_API_KEY")
open_router_key = os.getenv("OPENROUTER_API_KEY")
deepseek_key = os.getenv("DEEPSEEK_API_KEY")
# ====== 1) æ¨¡å‹é…ç½®ï¼ˆOpenAI å…¼å®¹ï¼‰ ======
OPENAI_API_KEY = qwen_key
OPENAI_BASE_URL = 'https://dashscope.aliyuncs.com/compatible-mode/v1'  # e.g. DashScope å…¼å®¹ç«¯ç‚¹
# æ¨èæ¨¡å‹ï¼ˆOpenAIï¼‰ï¼š"gpt-4o-mini" / "gpt-4.1-mini"
# æ¨èæ¨¡å‹ï¼ˆQwen å…¼å®¹ç«¯ç‚¹ï¼‰ï¼š"qwen2.5-32b-instruct" æˆ– "qwen2.5-7b-instruct"
LLM_MODEL = os.environ.get("LLM_MODEL", "qwen-plus-2025-07-28")

if not OPENAI_API_KEY:
    raise RuntimeError("è¯·å…ˆè®¾ç½® OPENAI_API_KEY")

# https://dashscope.aliyuncs.com/compatible-mode/v1
# qwen-plus-2025-07-28
# https://openrouter.ai/api/v1
# openai/gpt-5
# https://api.deepseek.com/v1
# deepseek-reasoner

# qwen3-max qwen-plus-2025-07-28
# llm = ChatOpenAI(
#     model="qwen3-max",          
#     api_key=qwen_key, 
#     base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
# )


llm = ChatOpenAI(
    model="deepseek-reasoner",          
    api_key=deepseek_key, 
    base_url="https://api.deepseek.com/v1"
)

# llm = ChatOpenAI(
#     model="openai/gpt-5",          
#     api_key=open_router_key, 
#     base_url="https://openrouter.ai/api/v1"
# )


# ====== 2) è¾“å‡ºæ•°æ®ç»“æ„ï¼ˆè®©ä»£ç†æŒ‰æ­¤ç»“æ„å JSONï¼‰ ======
class Job(BaseModel):
    title: str = Field(..., description="å²—ä½åç§°ï¼Œå¦‚ Javaå¼€å‘å·¥ç¨‹å¸ˆ/èµ„æ·±Java/åç«¯å·¥ç¨‹å¸ˆ")
    company: str = Field(..., description="å…¬å¸åç§°")
    salary: Optional[str] = Field(None, description="è–ªèµ„åŒºé—´ï¼Œå¦‚ 20-30KÂ·14è–ª/25-40K")
    location: Optional[str] = Field(None, description="å·¥ä½œåœ°ç‚¹/åŸå¸‚/åŒºå¿")
    experience: Optional[str] = Field(None, description="ç»éªŒè¦æ±‚ï¼Œå¦‚ 3-5å¹´/ä¸é™")
    education: Optional[str] = Field(None, description="å­¦å†è¦æ±‚ï¼Œå¦‚ æœ¬ç§‘/å¤§ä¸“/ä¸é™")
    source_page: Optional[str] = Field(None, description="æŠ“å–é¡µé¢URLç”¨äºæº¯æº")

# ====== 3) æŠ“å–ä»»åŠ¡ Promptï¼ˆè®©ä»£ç†åœ¨é¡µé¢å†…æ“ä½œã€æ»šåŠ¨å¹¶æŠ½å–ï¼‰ ======
def build_task(keyword: str, city_hint: str = "å…¨å›½", max_jobs: int = 30, n: int = 1) -> str:
    schema = Job.model_json_schema()
    return f"""
ä½ æ˜¯ä¸€ä¸ªç½‘é¡µé‡‡é›†ä»£ç†ã€‚è®¿é—® https://www.zhipin.com/web/geek/jobs?ka=header-jobs ï¼ˆBOSSç›´è˜ï¼‰ï¼Œç­‰å¾…é¡µé¢åŠ è½½å®Œæˆã€‚
æœç´¢ç»“æœé¡µé¢ç»“æ„ï¼š
- æœç´¢åœ¨ div class='job-search-form' , æœç´¢çš„è¾“å…¥åœ¨ (placeholder="æœç´¢èŒä½ã€å…¬å¸")
- èŒä½ç»“æœåˆ—è¡¨ å’Œ èŒä½è¯¦ç»†ä¿¡æ¯ æ˜¯å·¦å³ä¸¤åˆ—ï¼Œæœç´¢çš„èŒä½ç»“æœåˆ—è¡¨ åœ¨ div class='job-list-container'ï¼ŒèŒä½åˆ—è¡¨ä¸­æ¯ä¸€ä¸ªèŒä½ä¿¡æ¯åœ¨ class='card-area' ï¼Œæ¯ä¸ªèŒä½è¯¦ç»†ä¿¡æ¯åœ¨ div class='job-detail-box'ã€‚

åœ¨æœç´¢æ¡†è¾“å…¥â€œ{keyword}â€ï¼ŒåŸå¸‚é€‰æ‹©â€œ{city_hint}â€ï¼ˆè‹¥æ— æ³•é€‰åˆ™é»˜è®¤å½“å‰æ˜¾ç¤ºåŸå¸‚ï¼‰ï¼Œæ‰§è¡Œæœç´¢ã€‚
æ„å»ºä¸€ä¸ªJSON æ•°ç»„**ï¼Œæ•°ç»„å…ƒç´ çš„ JSON ç»“æ„ä¸¥æ ¼éµå¾ªä¸‹é¢çš„ JSON Schema
æœç´¢ç»“æœåŠ è½½å®Œæˆåï¼Œæ‰§è¡Œ{max_jobs}æ¬¡ä»¥ä¸‹è¦æ±‚æ“ä½œï¼š
1) ç‚¹å‡»èŒä½ç»“æœåˆ—è¡¨ä¸­çš„ç¬¬{n}ä¸ªæ¡èŒä½ä¿¡æ¯ï¼Œåœ¨èŒä½ç»“æœåˆ—è¡¨çš„å³è¾¹ä¼šåŠ è½½èŒä½çš„è¯¦ç»†ä¿¡æ¯ã€‚
2) å¯¹æ¯æ¡èŒä½æŠ½å–å­—æ®µï¼štitle, company, salary, location, experience, educationã€‚
3) æŠŠæ¯æ¡èŒä½æå–åˆ°çš„æ•°æ®æ”¾åˆ°å·²ç»æ„å»ºçš„JSON æ•°ç»„ä¸­ã€‚
4) n çš„å€¼ åŠ 1ã€‚

è¾“å‡º JSON æ•°ç»„ã€‚
{json.dumps(schema, ensure_ascii=False, indent=2)}

æ³¨æ„ï¼š
- é¿å…ç‚¹å‡»â€œèŠå¤©â€â€œæŠ•é€’â€ç­‰éœ€è¦ç™»å½•çš„æ“ä½œã€‚
- å°½é‡ä¸è·³è½¬è¯¦æƒ…é¡µï¼Œè‹¥éœ€è¦ä¹Ÿå¯ä»¥ç‚¹å¼€æ–°æ ‡ç­¾è¯»å–ä¿¡æ¯åè¿”å›ã€‚
- è¯·åŠ¡å¿…è¾“å‡º **çº¯ JSON**ï¼Œä¸è¦å¸¦ä»»ä½•æ³¨é‡Šã€é¢å¤–æ–‡æœ¬æˆ– Markdownã€‚
- æœ€å¤šæŠ½å– {max_jobs} æ¡è®°å½•ã€‚
"""

# ====== 4) è¿è¡Œä»£ç†å¹¶è§£æç»“æœä¸º CSV ======
def extract_json(text: str) -> List[dict]:
    """
    ä»£ç†ç»å¸¸ä¼šè¿”å›è§£é‡Š + JSONï¼Œæˆ‘ä»¬åªä¿ç•™ç¬¬ä¸€ä¸ªçº¯ JSON æ•°ç»„ã€‚
    """
    # å°è¯•æŠ“å–ç¬¬ä¸€ä¸ª JSON æ•°ç»„
    m = re.search(r"(\[\s*\{.*?\}\s*\])", text, flags=re.S)
    if not m:
        # ä¹Ÿå¯èƒ½å°±æ˜¯çº¯ JSON
        if text.strip().startswith("[") and text.strip().endswith("]"):
            return json.loads(text)
        raise ValueError("æœªåœ¨ä»£ç†è¾“å‡ºä¸­å‘ç° JSON æ•°ç»„")
    return json.loads(m.group(1))

def save_csv(rows: List[dict], out_path: str):
    if not rows:
        print("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
        return
    keys = ["title", "company", "salary", "location", "experience", "education", "source_page"]
    with open(out_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=keys)
        writer.writeheader()
        for r in rows:
            writer.writerow({k: r.get(k, "") for k in keys})
    print(f"âœ… å·²ä¿å­˜ CSV: {out_path} (å…± {len(rows)} æ¡)")

async def main():
    keyword = "Java å¼€å‘"
    city = "å…¨å›½"  # ä¹Ÿå¯ç”¨ï¼šåŒ—äº¬/ä¸Šæµ·/æ·±åœ³/æ­å· ç­‰
    task = build_task(keyword, city, max_jobs=10)

    # å»ºè®®è®¾ç½®ä¸€ä¸ªä½ çš„æœ¬åœ° Profile ç›®å½•ï¼Œèƒ½æ˜¾è‘—é™ä½è¢«é£æ§æ¦‚ç‡ï¼ˆå·²ç™»å½•æ›´ç¨³ï¼‰
    PROFILE_DATA_DIR = '/Users/gongwenwei/Library/Application Support/Google/Chrome/Profile 2'
    #os.makedirs(user_data_dir, exist_ok=True)

    # 5) æµè§ˆå™¨ä¸ä¸Šä¸‹æ–‡é…ç½®ï¼šä¸­æ–‡ã€éæ— å¤´ã€è¾ƒå¤§è§†å£ã€æŒä¹…åŒ– Profile
    # browser = Browser(
    #     config=BrowserConfig(
    #         headless=False,  # è°ƒè¯•é˜¶æ®µå»ºè®®å¯è§
    #         chromium_port=None,  # è®© browser-use è‡ªè¡Œç®¡ç†
    #     ),
    #     context_config=BrowserContextConfig(
    #         user_data_dir=user_data_dir,
    #         locale="zh-CN",
    #         viewport={"width": 1440, "height": 900},
    #         geolocation=None,  # å¦‚æœéœ€è¦å¯å¡«ä¸­å›½åŸå¸‚ç»çº¬åº¦
    #         timezone_id="Asia/Shanghai",
    #     ),
    # )

    # config = BrowserConfig(
    #     use_cloud=False,                     # âœ… ç¦ç”¨äº‘ç«¯æ¨¡å¼
    #     headless=False,                      # æ˜¯å¦æ˜¾ç¤ºæµè§ˆå™¨çª—å£
    #     browser_type="chromium",             # å¯é€‰: chromium / firefox / webkit
    #     user_data_dir=user_data_dir,   # ä¿å­˜ç™»å½•çŠ¶æ€ç­‰
    #     viewport={"width": 1440, "height": 900},
    #     slow_mo=100                          # æ¯ä¸ªæ“ä½œä¹‹é—´å»¶è¿Ÿ (ms)
    # )
    # browser = Browser(config=config)
    # TimeoutError: Event handler browser_use.browser.watchdog_base.BrowserSession.on_BrowserStartEvent#
    # https://github.com/browser-use/browser-use/issues/3196
    # https://developer.chrome.com/blog/remote-debugging-port?hl=zh-cn
    browser = Browser(
        use_cloud=False,
        headless=False,                     # æ˜¾ç¤ºæµè§ˆå™¨çª—å£
        # user_data_dir='~/Library/Application Support/Google/Chrome',
        # profile_directory='Profile 2',
        # viewport={"width": 1440, "height": 900}
    )



    # 6) åˆ›å»ºä»£ç†
    agent = Agent(
        task=task,
        browser=browser,
        max_actions=50,          # ç»™è¶³å¤Ÿçš„åŠ¨ä½œæ­¥æ•°ç”¨äºæ»šåŠ¨å’Œæå–
        max_failures=5,          # å®¹é”™
        llm=llm,
        # ä¸‹é¢ä¸¤ä¸ªå¯ä»¥è®©åŠ¨ä½œæ›´â€œåƒäººç±»â€ï¼Œé™ä½è¢«é£æ§é£é™©
        action_delay=1.2,        # æ¯æ­¥ä¹‹é—´åœé¡¿
        step_by_step=True,       # è®©æ¨¡å‹è§£é‡Šä¸€æ­¥æ­¥æ“ä½œï¼ˆæ›´ç¨³ï¼‰
        use_vision=False
    )

    print("ğŸš€ å¼€å§‹ä»»åŠ¡ï¼š", keyword)
    result = await agent.run()  # 0.7.x åŒæ­¥ APIï¼›è‹¥ä½ ç”¨çš„æ˜¯å¼‚æ­¥è¯·æ”¹æˆ await agent.run()

    # 7) è§£æè¾“å‡º
    final_text = result.final_result if hasattr(result, "final_result") else str(result)
    try:
        rows = extract_json(final_text)
    except Exception as e:
        print("â—è§£æå¤±è´¥ï¼ŒåŸå§‹è¾“å‡ºå¦‚ä¸‹ï¼š\n", final_text[:2000])
        raise e

    # 8) ä¿å­˜ CSV
    out_csv = f"zhipin_{keyword.replace(' ', '')}.csv"
    save_csv(rows, out_csv)

if __name__ == "__main__":
    asyncio.run(main())
